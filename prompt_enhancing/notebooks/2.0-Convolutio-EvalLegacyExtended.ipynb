{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "from typing import cast\n",
    "import random\n",
    "\n",
    "from archaeo_super_prompt.dataset import MagohDataset\n",
    "from archaeo_super_prompt.modeling import legacy_predict\n",
    "from archaeo_super_prompt.modeling.struct_extract.main_transformer import MagohDataExtractor\n",
    "from archaeo_super_prompt.visualization import mlflow_logging as mmlflow\n",
    "from archaeo_super_prompt.config.env import getenv_or_throw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Legacy pipeline evaluation over extended set\n",
    "\n",
    "We evaluate the first complete pipeline (with one single, not-helped DSPy model) over a set of interventions with **digital-native** PDF documents and also ones with **scanned** PDF files, now that a vision LLM ingests the PDF documents during the pre-processing stage.\n",
    "\n",
    "## Expectations\n",
    "\n",
    "The legacy pipeline has not a context very suitable, judging from the knowledge we have about the data and the fields to be predicted. Moreover, to run the evaluation in a rational duration, we only read the 5 first pages of the documents, as an heuristic states that the incipit contains most of the metadata to be extracted. Nonetheless, we expect because of that limited per-field accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## MLFLow experiment setup\n",
    "\n",
    "Be sure to have run the mlflow server with this command at in the `prompt_enhancing/` directory\n",
    "\n",
    "```sh\n",
    "just serve-tracer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"Legacy Model - Extended Documents\"\n",
    "mlflow.set_tracking_uri(f\"http://{getenv_or_throw(\"MLFLOW_HOST\")}:{getenv_or_throw(\"MLFLOW_PORT\")}\")\n",
    "mlflow.set_experiment(EXP_NAME)\n",
    "mlflow.dspy.autolog()\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Sample selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_digitally_born_documents = [\n",
    "    # very good\n",
    "    33799, 34439, 38005, 36837, 36937, 37614, 37026, 37971, 36846, 36304, 34423, 36052,\n",
    "    37043, 36554, 989, 37007, 30897, 36351, 36308, 38013, 36011, 33828, 1221,\n",
    "    38039, 35429, 37065, 37116, 34452, 33441, 33062, 34939, 35918, 33689, 34508, 31035,\n",
    "    38220, 38092, 36979, 36854, 36207, 34915, 35688, 36359,\n",
    "    # not that good\n",
    "    31164, 32600, 33760, 32714, 31208, 30712,\n",
    "    ]\n",
    "ok_scanned_pdfs = {\n",
    "    32666, 31298, 33548, 35189, 35399, 30925, 37040, 37379, 33589, 34769,\n",
    "    33858, 34329, 5193, 37706, 30647, 37702, 33540, 36042, 33357, 34959,\n",
    "    30646, 33547, 32581, 30878, 37302, 33560, 35881, 31031, 37381, 242, 34869,\n",
    "    33841, 36465, 33499, 36095, 36068, 33594, 33904, 33644, 33553, 35052,\n",
    "    33630, 34426, 31090, 30716, 31059, 35849, 33813, 34666, 36119, 830,\n",
    "    36187, 31977, 34787, 33749, 35447, 33555, 33846, 34093, 33508, 33710\n",
    "}\n",
    "dirty_pdfs = {\n",
    "    36648, 32433, 35131, 33383, 30657, 31312, 30399, 33331, 31234, 30548,\n",
    "    34685, 34237, 35114, 30821, 33708, 33668, 34932, 30697, 38241, 33443,\n",
    "    37305, 33535, 31815, 35203, 33576, 32053, 33761, 37910, 35983, 31314,\n",
    "    37400, 36457, 33582, 31903, 32494, 33184, 36070, 31804, 30861\n",
    "}\n",
    "\n",
    "selected_ids = set(_digitally_born_documents).union(ok_scanned_pdfs, dirty_pdfs)\n",
    "ds = MagohDataset(selected_ids)\n",
    "print(\"Intervention number:\", ds.intervention_data)\n",
    "inputs = ds.files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Pipeline run\n",
    "\n",
    "We use a dataframe-suitable version of the scikit-learn pipelines to pipe each module in this order :\n",
    "\n",
    "- ocr\n",
    "- layout text reading + chunking\n",
    "- strucured data extraction\n",
    "\n",
    "The LLM calls are traced by the MLFlow intergration and are viewable within links displayed by the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = legacy_predict.get_legacy_model()\n",
    "extraction_model = cast(MagohDataExtractor, pipeline.named_steps[\"extractor\"])\n",
    "input_example = ds.get_files_for_batch([\n",
    "    random.sample(sorted(selected_ids), 1)[0]\n",
    "])\n",
    "\n",
    "# text scan + chunking\n",
    "intermediate_inputs = pipeline.named_steps[\"vllm\"].transform(inputs)\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # TODO: change the evaluation\n",
    "    # mmlflow.save_models(pipeline, input_example)\n",
    "    score_value = extraction_model.score(intermediate_inputs, ds)\n",
    "    score_results = extraction_model.score_results\n",
    "    mmlflow.save_metric_scores(score_value, score_results)\n",
    "    mmlflow.save_table_in_artifacts(score_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Evaluation result inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from archaeo_super_prompt.visualization import (\n",
    "        init_complete_vizualisation_engine, run_display_server\n",
    ")\n",
    "\n",
    "init_complete_vizualisation_engine(score_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_display_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "score_results.to_csv(str(Path(\"./results.csv\").resolve()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Poetry",
   "language": "python",
   "name": "poetry-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
