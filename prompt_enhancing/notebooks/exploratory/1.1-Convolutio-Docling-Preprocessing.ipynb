{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from docling.datamodel.pipeline_options_vlm_model import ResponseFormat\n",
    "from pathlib import Path\n",
    "\n",
    "from archaeo_super_prompt.dataset.load import MagohDataset\n",
    "from archaeo_super_prompt.modeling.pdf_to_text.stream_ocr_manual import (\n",
    "    converter,\n",
    "    vllm_vlm_options\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# ðŸ¦†ðŸ“ƒ PDF complete ingestion with Docling preprocessing\n",
    "\n",
    "We try the young framework Docling and its usage of VLLM on the HuggingFace repositories to achieve thoses tasks:\n",
    "\n",
    "- document OCR with Italian language analysis (VLLM)\n",
    "- document chunking with these features:\n",
    "    - layout-aware\n",
    "    - smart tokenization\n",
    "\n",
    "All of these things are possible with incorporating several open ML models into the Docling pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SAMPLES_FETCHED = 300\n",
    "SEED = 0.5\n",
    "\n",
    "_selected_ids = [35983, 31298]\n",
    "dataset = MagohDataset(_selected_ids)\n",
    "selected_ids = set(_selected_ids)\n",
    "inputs = dataset.get_files_for_batch(selected_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"filepath\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    TIMEOUT_PER_PAGE = 60 * 3\n",
    "    # Example using the Granite Vision model with VLLM:\n",
    "\n",
    "    doc_converter = converter(\n",
    "        vllm_vlm_options(\n",
    "            model=\"ibm-granite/granite-vision-3.3-2b\",\n",
    "            prompt=\"OCR the full page for markdown-based processing.\",\n",
    "            # Doctags is only supported by doclings vllm for now\n",
    "            response_format=ResponseFormat.MARKDOWN,\n",
    "            allowed_timeout=TIMEOUT_PER_PAGE,\n",
    "        )\n",
    "    )\n",
    "    results = [\n",
    "        content for content in doc_converter.convert_all(\n",
    "            [Path(p.filepath) for p in inputs.itertuples()],\n",
    "            page_range=(1, 2), raises_on_error=False)\n",
    "    ]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Inspect the results\n",
    "\n",
    "We export into markdown the results for display it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(result[1][1][0].export_to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.chunking import HierarchicalChunker\n",
    "\n",
    "chunker = HierarchicalChunker()\n",
    "chunk_iter = chunker.chunk(dl_doc=result[0][1][0])\n",
    "chunks = list(chunk_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"=== {i} ===\")\n",
    "    txt_tokens = len(\n",
    "        chunk.text.rstrip().split(\" \")\n",
    "    )  # tokenizer.count_tokens(chunk.text)\n",
    "    print(f\"chunk.text ({txt_tokens} tokens):\\n{chunk.text!r}\")\n",
    "\n",
    "    ser_txt = chunker.contextualize(chunk=chunk)\n",
    "    ser_tokens = len(\n",
    "        ser_txt.rstrip().split(\" \")\n",
    "    )  # tokenizer.count_tokens(ser_txt)\n",
    "    print(f\"chunker.contextualize(chunk) ({ser_tokens} tokens):\\n{ser_txt!r}\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(result[1][1][0].export_to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Hybrid-Chunking\n",
    "\n",
    "This enable to use a token-aware chunking after a first layout-aware chunking, from a tokenizer built from the embedding model that will be used for the chunk selection, the thesaurus match comparisons, etc.\n",
    "\n",
    "This has two advantages:\n",
    "1. The chunks will be recutted more suitably for a procesing by the embedding model\n",
    "2. The chunks might be recutted more shortly without losing contextual information, even if the layout was originally set of big paragraphs which does not cut the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.chunking import HybridChunker\n",
    "\n",
    "from docling_core.transforms.chunker.tokenizer.huggingface import (\n",
    "    HuggingFaceTokenizer,\n",
    ")\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "EMBED_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "MAX_TOKENS = 512  # set to a small number for illustrative purposes\n",
    "\n",
    "tokenizer = HuggingFaceTokenizer(\n",
    "    tokenizer=AutoTokenizer.from_pretrained(EMBED_MODEL_ID),\n",
    "    max_tokens=MAX_TOKENS,  # optional, by default derived from `tokenizer` for HF case\n",
    ")\n",
    "chunker = HybridChunker(tokenizer=tokenizer)\n",
    "chunk_iter = chunker.chunk(dl_doc=result[1][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_chunk_list = list(chunk_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(hybrid_chunk_list):\n",
    "    print(f\"=== {i} ===\")\n",
    "    txt_tokens = len(\n",
    "        chunk.text.rstrip().split(\" \")\n",
    "    )  # tokenizer.count_tokens(chunk.text)\n",
    "    print(f\"chunk.text ({txt_tokens} tokens):\\n{chunk.text!r}\")\n",
    "\n",
    "    ser_txt = chunker.contextualize(chunk=chunk)\n",
    "    ser_tokens = len(\n",
    "        ser_txt.rstrip().split(\" \")\n",
    "    )  # tokenizer.count_tokens(ser_txt)\n",
    "    print(f\"chunker.contextualize(chunk) ({ser_tokens} tokens):\\n{ser_txt!r}\")\n",
    "\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Poetry",
   "language": "python",
   "name": "poetry-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
