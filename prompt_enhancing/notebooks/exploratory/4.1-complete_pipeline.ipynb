{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stuff to run before this notebook\n",
    "\n",
    "# local pc, make sure postgresql is running with the necessary data\n",
    "# ssh -i ./fair_key -R 5432:localhost:5432 -L 8889:localhost:8889 -L 8887:localhost:8887 -L 8050:localhost:8050 name@host\n",
    "\n",
    "# for conda env and vllm venv\n",
    "\n",
    "# conda activate /raid/ggattiglia/magoh_ai/env\n",
    "# source ./vllm-server/.venv/bin/activate\n",
    "\n",
    "# LLM, needs conda and venv, replace cuda devices with gpus and hf token with your token (need to request permission to use model on huggingface first)\n",
    "# CUDA_VISIBLE_DEVICES=0,1 HF_TOKEN=[hf_token] vllm serve --port 8001 google/gemma-3-27b-it --tensor-parallel-size 2 --gpu-memory-utilization 0.6\n",
    "\n",
    "# mlflow, needs conda\n",
    "# mlflow server --host 127.0.0.1 --port 8887\n",
    "\n",
    "# NER server, needs conda and venv\n",
    "# cd prompt_enhancing/models/custom-remote-models/src/magoh_ai_sup_server\n",
    "# just run-server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the functions and code in this notebook should be in the code itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.path.join(os.getcwd(), '..', '..', 'src'))\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from archaeo_super_prompt.dataset import MagohDataset, SamplingParams\n",
    "import archaeo_super_prompt.modeling.train as training\n",
    "import archaeo_super_prompt.modeling.predict as infering\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from archaeo_super_prompt.visualization import mlflow_logging as mmlflow\n",
    "from archaeo_super_prompt import visualization as visualizator\n",
    "from archaeo_super_prompt.config.env import getenv_or_throw\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import set_config\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from archaeo_super_prompt.utils.cache import get_cache_dir_for\n",
    "\n",
    "class LoadScans(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cache_csv: Path):\n",
    "        self.cache_csv = Path(cache_csv)\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        scans = pd.read_csv(self.cache_csv)\n",
    "        scans = scans.drop_duplicates(subset=[\"id\"])\n",
    "        return X.merge(scans, on=\"id\", how=\"inner\")\n",
    "\n",
    "# cache location\n",
    "CACHE_CSV = get_cache_dir_for(\"interim\", \"miscel\") / \"scans.csv\"\n",
    "SCANS_DF = pd.read_csv(CACHE_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"Complete training test\"\n",
    "mlflow.set_tracking_uri(f\"http://{getenv_or_throw('MLFLOW_HOST')}:{getenv_or_throw('MLFLOW_PORT')}\")\n",
    "mlflow.set_experiment(EXP_NAME)\n",
    "mlflow.dspy.autolog(log_compiles=True, log_evals=True, log_traces_from_compile=True)\n",
    "pd.set_option('display.max_columns', None)\n",
    "set_config(display=\"diagram\")\n",
    "\n",
    "from archaeo_super_prompt.utils.cache import get_cache_dir_for\n",
    "store_dir = get_model_store_dir()\n",
    "Path(store_dir).mkdir(parents=True, exist_ok=True)\n",
    "print(\"Model store:\", store_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HARD RESET OF CACHES / ARTIFACTS run this when fresh mlflow rerun is needed\n",
    "import os, shutil, pathlib, urllib.parse, uuid\n",
    "import mlflow\n",
    "\n",
    "# wipe compiled DSPy programs used by extractors\n",
    "from archaeo_super_prompt.utils.result import get_model_store_dir\n",
    "shutil.rmtree(get_model_store_dir(), ignore_errors=True)\n",
    "\n",
    "# wipe joblib / skdag caches inside project cache dirs\n",
    "from archaeo_super_prompt.utils.cache import get_cache_dir_for\n",
    "for scope in [\"external\",\"internal\",\"interim\",\"miscel\",\"thesaurus\",\"raw\"]:\n",
    "    try:\n",
    "        base = get_cache_dir_for(scope)\n",
    "    except Exception:\n",
    "        continue\n",
    "    if not base.exists(): \n",
    "        continue\n",
    "    for p in base.rglob(\"*\"):\n",
    "        if p.is_dir() and any(k in p.name for k in (\"joblib\",\"skdag\",\"__joblib_cache__\")):\n",
    "            shutil.rmtree(p, ignore_errors=True)\n",
    "\n",
    "fresh_dir = pathlib.Path.cwd() / f\"mlruns_fresh_{uuid.uuid4().hex[:6]}\"\n",
    "fresh_dir.mkdir(parents=True, exist_ok=True)\n",
    "mlflow.set_tracking_uri(f\"http://{getenv_or_throw('MLFLOW_HOST')}:{getenv_or_throw('MLFLOW_PORT')}\")\n",
    "mlflow.set_experiment(f\"fresh-{uuid.uuid4().hex[:6]}\")\n",
    "mlflow.dspy.autolog(log_compiles=True, log_evals=True, log_traces_from_compile=True)\n",
    "pd.set_option('display.max_columns', None)\n",
    "set_config(display=\"diagram\")\n",
    "\n",
    "store_dir = get_model_store_dir()\n",
    "Path(store_dir).mkdir(parents=True, exist_ok=True)\n",
    "print(\"Model store:\", store_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from archaeo_super_prompt.dataset import MagohDataset\n",
    "\n",
    "selected_ids = set(map(int, SCANS_DF[\"id\"].dropna().tolist()))\n",
    "ds = MagohDataset(selected_ids)\n",
    "\n",
    "inputs = ds.files.merge(SCANS_DF[[\"id\"]].drop_duplicates(), on=\"id\", how=\"inner\")\n",
    "train_inputs, eval_inputs = inputs.iloc[:10], inputs.iloc[10:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, importlib\n",
    "\n",
    "#delete openai in env if present, we have a local model\n",
    "for k in (\"OPENAI_BASE_URL\", \"OPENAI_API_BASE\"):\n",
    "    os.environ.pop(k, None)\n",
    "\n",
    "#should be moved to env\n",
    "os.environ[\"VLLM_SERVER_BASE_URL\"] = \"http://127.0.0.1:8001/v1\"\n",
    "#dspy seems to fall back to openai in case of errors, this is a dummy key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-local\"\n",
    "\n",
    "from archaeo_super_prompt.modeling.struct_extract import language_model as lm_provider_mod\n",
    "from archaeo_super_prompt.modeling.struct_extract import field_extractor as fe\n",
    "lm_provider_mod = importlib.reload(lm_provider_mod)\n",
    "fe = importlib.reload(fe)\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from archaeo_super_prompt.utils.cache import get_cache_dir_for\n",
    "from archaeo_super_prompt.modeling import pdf_to_text\n",
    "import ast\n",
    "\n",
    "CACHE_CSV = get_cache_dir_for(\"interim\", \"miscel\") / \"scans.csv\"\n",
    "\n",
    "def _as_list(v):\n",
    "    if isinstance(v, list): return v\n",
    "    if pd.isna(v): return []\n",
    "    if isinstance(v, str):\n",
    "        s=v.strip()\n",
    "        if s and s[0] in \"[{(\":\n",
    "            try:\n",
    "                x=ast.literal_eval(s)\n",
    "                return x if isinstance(x, list) else [x]\n",
    "            except Exception:\n",
    "                return [v]\n",
    "        return [v]\n",
    "    return [v]\n",
    "\n",
    "def _as_str_list(v):\n",
    "    return [str(x) for x in _as_list(v)]\n",
    "\n",
    "def _as_int_list(v):\n",
    "    out=[]\n",
    "    for x in _as_list(v):\n",
    "        try:\n",
    "            out.append(int(x))\n",
    "        except Exception:\n",
    "            try:\n",
    "                out.append(int(float(x)))\n",
    "            except Exception:\n",
    "                pass\n",
    "    return out\n",
    "\n",
    "class LoadScans(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cache_csv: str | Path):\n",
    "        self.cache_csv = Path(cache_csv)\n",
    "        self._df = None\n",
    "    def fit(self, X, y=None):\n",
    "        df = pd.read_csv(self.cache_csv)\n",
    "        if \"id\" in df.columns:\n",
    "            df[\"id\"] = df[\"id\"].astype(\"int64\").astype(int)\n",
    "        if \"chunk_type\" in df.columns:\n",
    "            df[\"chunk_type\"] = df[\"chunk_type\"].apply(_as_str_list)\n",
    "        if \"chunk_page_position\" in df.columns:\n",
    "            df[\"chunk_page_position\"] = df[\"chunk_page_position\"].apply(_as_int_list)\n",
    "        if \"identified_thesaurus\" in df.columns:\n",
    "            df[\"identified_thesaurus\"] = df[\"identified_thesaurus\"].apply(_as_int_list)\n",
    "        if \"named_entities\" in df.columns:\n",
    "            df[\"named_entities\"] = df[\"named_entities\"].apply(_as_list)\n",
    "        self._df = df\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        if \"id\" in X.columns:\n",
    "            X[\"id\"] = X[\"id\"].astype(int)\n",
    "        return X.merge(self._df, on=\"id\", how=\"inner\")\n",
    "\n",
    "# replace vision lm with preprocessed scans\n",
    "pdf_to_text.VLLM_Preprocessing = lambda **kw: LoadScans(CACHE_CSV)\n",
    "\n",
    "training = importlib.reload(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_base_parts = training.get_training_dag()\n",
    "from archaeo_super_prompt.modeling import predict as infering\n",
    "expected_final_pipeline = infering.build_complete_inference_dag(_base_parts)\n",
    "expected_final_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, datetime, pandas as pd\n",
    "from archaeo_super_prompt.modeling.struct_extract.extractors.archiving_date import ArchivingDateProvider, ArchivingDateOutputSchema\n",
    "\n",
    "def _predict_safe(self, X):\n",
    "    def parse(dp):\n",
    "        if dp is None: return datetime.date(1900,1,1)\n",
    "        if isinstance(dp, (datetime.date, datetime.datetime, pd.Timestamp)):\n",
    "            return dp.date() if hasattr(dp, \"date\") else dp\n",
    "        s = str(dp).strip()\n",
    "        if s == \"\" or s.lower() in (\"none\", \"nan\", \"nat\"): return datetime.date(1900,1,1)\n",
    "        for fmt in (\"%Y-%m-%d\",\"%d-%m-%Y\",\"%d/%m/%Y\",\"%Y/%m/%d\",\"%d.%m.%Y\",\"%Y.%m.%d\"):\n",
    "            try: return datetime.datetime.strptime(s, fmt).date()\n",
    "            except Exception: pass\n",
    "        parts = re.split(r\"[-/\\. ]+\", s)\n",
    "        try:\n",
    "            if len(parts) >= 3:\n",
    "                d, m, y = map(int, parts[:3])\n",
    "                if y < 100: y += 2000\n",
    "                return datetime.date(y, m, d)\n",
    "        except Exception: pass\n",
    "        m = re.search(r\"(\\d{4})\", s)\n",
    "        if m: return datetime.date(int(m.group(1)), 1, 1)\n",
    "        return datetime.date(1900,1,1)\n",
    "    rows = [{\"id\": a.id, \"data_protocollo\": parse(getattr(a, \"building__Data_Protocollo\", None))}\n",
    "            for a in self._mds.get_answers(set(X[\"id\"].to_list()))]\n",
    "    return ArchivingDateOutputSchema.validate(pd.DataFrame(rows).set_index(\"id\"))\n",
    "\n",
    "ArchivingDateProvider.predict = _predict_safe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, calendar, datetime\n",
    "import archaeo_super_prompt.modeling.struct_extract.extractors.intervention_date as ide\n",
    "from archaeo_super_prompt.modeling.struct_extract.extractors.intervention_date import InterventionStartExtractor, ITALIAN_MONTHS, Data, DataInterventoInputData\n",
    "from archaeo_super_prompt.modeling.struct_extract.extractors.comune import ComuneExtractor, ComuneInputData, Comune\n",
    "from archaeo_super_prompt.dataset.thesauri import comune_province as cp\n",
    "\n",
    "def load_comune_aligned():\n",
    "    df = pd.read_csv(cp._get_comune_file())\n",
    "    df = df[df[\"nome\"].notnull() & df[\"provincia\"].notnull()]\n",
    "    return [(id_com, nome) for _, id_com, nome in df[[\"id_com\",\"nome\"]].itertuples()]\n",
    "\n",
    "cp.load_comune = load_comune_aligned\n",
    "\n",
    "\n",
    "DATE_RE = re.compile(r\"\\b(\\d{1,2}[\\/\\.-]\\d{1,2}[\\/\\.-]\\d{2,4}|gennaio|febbraio|marzo|aprile|maggio|giugno|luglio|agosto|settembre|ottobre|novembre|dicembre)\\b\", re.I)\n",
    "\n",
    "# remove bad ocr output\n",
    "def _strip_tables_and_noise(s):\n",
    "    s=re.sub(r\"^\\s*\\d+\\s*,\\s*\\d+\\s*=\\s*.*$\", \"\", s, flags=re.M|re.S)\n",
    "    s=re.sub(r\"`-+`\\s*\", \"\", s)\n",
    "    return s\n",
    "\n",
    "# avoids going over max context window for long (and badly read) documents\n",
    "def _focus_and_truncate(s, max_chars=20000):\n",
    "    s=_strip_tables_and_noise(s)\n",
    "    if len(s)<=max_chars: return s\n",
    "    lines=s.splitlines()\n",
    "    hits=[ln for ln in lines if DATE_RE.search(ln)]\n",
    "    head=\"\\n\".join(lines[:4000])\n",
    "    tail=\"\\n\".join(lines[-2000:])\n",
    "    middle=\"\\n\".join(hits)[:8000]\n",
    "    out=\"\\n\".join([head,middle,tail])\n",
    "    return out[:max_chars]\n",
    "\n",
    "# feed trimmed \"fragmenti relazione\" and fix month (went out of bounds)\n",
    "def _to_dspy_input_patched(self, x):\n",
    "    d=x.data_protocollo\n",
    "    return DataInterventoInputData(\n",
    "        fragmenti_relazione=_focus_and_truncate(x.merged_chunks),\n",
    "        data_di_archiviazone=Data(\n",
    "            giorno=int(getattr(d,\"day\",d.day)),\n",
    "            mese=ITALIAN_MONTHS[int(getattr(d,\"month\",d.month))-1],\n",
    "            anno=int(getattr(d,\"year\",d.year)),\n",
    "        ),\n",
    "    )\n",
    "# replace intervention start extractor to not crash when context is too large\n",
    "InterventionStartExtractor._to_dspy_input = _to_dspy_input_patched\n",
    "\n",
    "# fixes for invalid model outputs that broke the pipeline\n",
    "def _as_int(x,d): \n",
    "    try: return int(x)\n",
    "    except Exception: return d\n",
    "def _clamp_month(m):\n",
    "    m=_as_int(m,1)\n",
    "    return 1 if m<1 else 12 if m>12 else m\n",
    "def _clamp_day(y,m,d):\n",
    "    y=_as_int(y,1900); m=_clamp_month(m); d=_as_int(d,1)\n",
    "    last=calendar.monthrange(max(1,y),m)[1]\n",
    "    return 1 if d<1 else last if d>last else d\n",
    "def _get_min_date_safe(o):\n",
    "    y=_as_int(getattr(o,\"start_year\",1900),1900)\n",
    "    m=_clamp_month(getattr(o,\"start_month\",1))\n",
    "    d=_clamp_day(y,m,getattr(o,\"start_day\",1))\n",
    "    return datetime.date(y,m,d)\n",
    "def _get_max_date_safe(o):\n",
    "    y=getattr(o,\"end_year\",None)\n",
    "    if y is None: y=getattr(o,\"start_year\",1900)\n",
    "    y=_as_int(y,1900)\n",
    "    m=_clamp_month(getattr(o,\"end_month\",12))\n",
    "    d=_clamp_day(y,m,getattr(o,\"end_day\",28))\n",
    "    return datetime.date(y,m,d)\n",
    "ide._get_min_date = _get_min_date_safe\n",
    "ide._get_max_date = _get_max_date_safe\n",
    "\n",
    "# comune fixing section\n",
    "\n",
    "def _read_comuni_tables_strict():\n",
    "    comuni = pd.read_csv(cp._get_comune_file())[\n",
    "        [\"id_com\", \"nome\", \"provincia\"]\n",
    "    ].rename(columns={\"id_com\":\"comune_id\",\"nome\":\"name\",\"provincia\":\"province_id\"})\n",
    "    comuni = comuni.dropna(subset=[\"name\",\"province_id\"]).copy()\n",
    "    comuni[\"comune_id\"] = comuni[\"comune_id\"].astype(int)\n",
    "    comuni[\"province_id\"] = comuni[\"province_id\"].astype(int)\n",
    "\n",
    "    # drop duplicated comuni keeping the first occurrence\n",
    "    dups = comuni[\"comune_id\"].duplicated(keep=\"first\").sum()\n",
    "    if dups:\n",
    "        print(f\"[warn] dropping {dups} duplicated comuni by comune_id\")\n",
    "        comuni = comuni.drop_duplicates(subset=[\"comune_id\"], keep=\"first\")\n",
    "\n",
    "    province = pd.read_csv(cp._get_provincie_file())[\n",
    "        [\"id_prov\",\"nome\",\"sigla\"]\n",
    "    ].rename(columns={\"id_prov\":\"province_id\",\"nome\":\"province_name\"})\n",
    "    province[\"province_id\"] = province[\"province_id\"].astype(int)\n",
    "    province = province.drop_duplicates(subset=[\"province_id\"], keep=\"first\")\n",
    "\n",
    "    # each comune refers to exactly one province\n",
    "    merged = comuni.merge(\n",
    "        province[[\"province_id\",\"province_name\",\"sigla\"]],\n",
    "        on=\"province_id\",\n",
    "        how=\"left\",\n",
    "        validate=\"m:1\",   # raise if a province_id maps to multiple province rows\n",
    "        copy=False,\n",
    "    )\n",
    "\n",
    "    comuni = comuni.set_index(\"comune_id\").sort_index()\n",
    "    merged = merged.set_index(\"comune_id\").sort_index()\n",
    "    return comuni, merged\n",
    "\n",
    "_COMUNI, _MERGED = _read_comuni_tables_strict()\n",
    "_POS2ID = _COMUNI.index.to_list()\n",
    "\n",
    "_FIELDS = set(Comune.model_fields.keys())\n",
    "\n",
    "def _mk_from_pos(pos: int):\n",
    "    if not (0 <= pos < len(_POS2ID)):\n",
    "        return None\n",
    "    cid = _POS2ID[pos]\n",
    "    if cid not in _MERGED.index:\n",
    "        return None\n",
    "    # scalar gets:\n",
    "    name = str(_MERGED.at[cid, \"name\"])\n",
    "    prov_name = str(_MERGED.at[cid, \"province_name\"])\n",
    "    sigla = str(_MERGED.at[cid, \"sigla\"])\n",
    "    kw = {}\n",
    "    if \"citta_nome\" in _FIELDS:      kw[\"citta_nome\"] = name\n",
    "    if \"provicia_nome\" in _FIELDS:   kw[\"provicia_nome\"] = prov_name\n",
    "    if \"provincia_sigla\" in _FIELDS: kw[\"provincia_sigla\"] = sigla\n",
    "    if \"id\" in _FIELDS:              kw[\"id\"] = int(cid)\n",
    "    # not sure if name or nome is used\n",
    "    if \"nome\" in _FIELDS and \"citta_nome\" not in _FIELDS: kw[\"nome\"] = name\n",
    "    if \"name\" in _FIELDS and \"citta_nome\" not in _FIELDS: kw[\"name\"] = name\n",
    "    return Comune(**kw)\n",
    "\n",
    "def _comune_to_dspy_input(self, x):\n",
    "    pos_list = getattr(x, \"identified_thesaurus\", None) or getattr(x, \"possibili_comuni\", None) or []\n",
    "    if not isinstance(pos_list, list): pos_list = [pos_list]\n",
    "    pos_list = [int(v) for v in pos_list if str(v).isdigit()]\n",
    "    cands = [c for c in (_mk_from_pos(p) for p in pos_list) if c is not None]\n",
    "    return ComuneInputData(\n",
    "        fragmenti_relazione=getattr(x, \"merged_chunks\", \"\"),\n",
    "        possibili_comuni=cands,\n",
    "    )\n",
    "\n",
    "# replace comune extractor code here because the original didn't work\n",
    "ComuneExtractor._to_dspy_input = _comune_to_dspy_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from archaeo_super_prompt.modeling import predict as infering\n",
    "with mlflow.start_run():\n",
    "    trained_dag_parts = training.train_from_scratch(train_inputs, ds)\n",
    "    per_field_scores, detailed_results = infering.score_dag(trained_dag_parts, eval_inputs, ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizator.init_complete_vizualisation_engine(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizator.run_display_server()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Poetry",
   "language": "python",
   "name": "poetry-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
